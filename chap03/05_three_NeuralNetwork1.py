import numpy as np

# 입력층에서 layer1으로 신호 전달
X = np.array([1.0, 0.5]) # 대문자로 쓰여진것은 행렬을 의미한다.
W1 = np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape) # (2, 3)
print(X.shape)  # (2,)
print(B1.shape) # (3,)

A1 = np.dot(X, W1) + B1 # 내적(스칼라) 내적을 계산한 값의 결과는 실수 즉 스칼라 값이 나온다.
print(A1) # [0.3 0.7 1.1]

# 1층에서의 활성화 함수 처리
def sigmoid(x):
    return 1 /(1+np.exp(-x))

Z1 = sigmoid(A1)
print(Z1) # [0.57444252 0.66818777 0.75026011]

# sigmoid 함수를 쓰는 이유 : 아무리 큰수, 아무리 작은수를 활성화 함수에 넣어도 1보다작고 0보다 크다. 범위를 축소화 해준다.
# ReLu 함수를 적용할 시  x가 양수일 경우는 x, 음수값일 경우  0으로 반환해준다.
# 하나의 값이 상대적으로 크면 단계를 거치면 거칠수록 큰 값이 점점더 부각되어 다른 것들의 값의 차이점을 보이지 않게된다.
# 입력된 신호들이 레이어를 거치면 거칠수록 점점 더 부각되어 지도록 하기위하여 활성화 함수를 사용한다.

# 1층에서 2층으로의 신호 전달
W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])
B2 = np.array([0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)


# 2층에서 3층(출력층)으로의 신호 전달
W3 = np.array([[0.1,0.3],[0.2,0.4]])
B3 = np.array([0.1,0.2])

# 항등 함수의 정의 - 출력단
def identity_function(x):
    return x


A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)


print(Y) # [0.31682708 0.69627909]

# 마지막 층에서는 활성화 함수를 사용할 필요 없다. 최종신호를 출력하면 된다.
# 분류분석의 최종적인 출력은 클래스를 구분